---
title: "Make Your Acceptance Criteria So Clear That AI Understands"
chapter: 31
lesson: 2
duration: "2 hours"
skills:
  - name: "Acceptance Criteria Writing"
    proficiency: "B1"
    category: "Technical"
  - name: "Testability Design"
    proficiency: "B1"
    category: "Technical"
  - name: "Machine-Executable Specifications"
    proficiency: "B1"
    category: "Technical"
learning_objectives:
  - "Write acceptance criteria that are unambiguous and machine-executable (B1)"
  - "Structure criteria for AI comprehension without human context (B1)"
  - "Design testable conditions that verify implementation correctness (B1)"
---

# Make Your Acceptance Criteria So Clear That AI Understands

## What You're About to Discover

The most important discovery in specification-driven development is this: **vague acceptance criteria produce vague code. Clear acceptance criteria produce usable code.**

You're going to experience this directly. You'll write vague criteria, ask your companion to build from them, see what goes wrong, then refine the criteria. With each refinement, the code gets better. You'll understand—not intellectually, but *experientially*—why clarity matters.

---

## Part 1: Write Vague Acceptance Criteria

Let's start with a grading system component: the ability to generate feedback.

Here's how you'd *naturally* describe it (vaguely):

```
The system should give feedback to students.
It should be accurate.
It should be helpful.
It should work quickly.
```

**Open your companion and ask it to build based on these vague criteria:**

```
I want a function that gives feedback to students
based on their assignment submissions.

Requirements:
- The system should give feedback to students
- It should be accurate
- It should be helpful
- It should work quickly

Write me Python code that does this.
Can you use some realistic sample data?
```

Your companion will generate something like this:

```python
def generate_feedback(submission: str) -> str:
    """Generate feedback for a student submission."""
    if len(submission) < 100:
        return "Your submission is too short. Write more."
    elif len(submission) > 2000:
        return "Your submission is too long. Be more concise."
    else:
        return "Good work. Keep improving."
```

**Look at what you got**: The code works. It runs. It gives feedback.

But ask yourself:

- **Is this accurate?** Accurate according to what criteria?
- **Is this helpful?** It's generic—any submission between 100-2000 words gets "Good work."
- **Is this quick?** Yes, it's very quick. But is speed the right metric? (Probably not for grading.)

Your companion followed your vague spec to the letter. And the result is useless.

---

## Part 2: Analyze What Went Wrong

**Ask your companion:**

```
Look at that feedback function. Why is it not very useful?
What information would I need to give you to make it better?
```

Your companion will say something like:

> "The feedback is too generic. I don't know:
> - What criteria should be evaluated (grammar? clarity? logic?)
> - How should those criteria be scored?
> - Should feedback be different for essays vs. code?
> - What does 'helpful' actually mean to you?
> - Should feedback be generated by AI, or from templates?
> - What's the format of feedback? One sentence? Paragraph?
>
> I just used the most basic heuristics I could think of
> because I didn't know what you actually wanted."

This is the key insight: **Your companion isn't being lazy. You were being vague.**

---

## Part 3: Refine Using the SMART Framework

Now you're going to apply the **SMART criteria** framework to fix this. SMART stands for:

- **S (Specific)**: What exactly should this do? (Not "accurate," but specifically what?)
- **M (Measurable)**: How would you test it? (Not "helpful," but what would we measure?)
- **A (Achievable)**: Is this realistic? (Not requiring AI telepathy)
- **R (Relevant)**: Does this match the real problem?
- **T (Time-bound)**: Should this have a deadline or performance limit?

Let's rebuild the acceptance criteria:

**Vague**:
```
The system should give feedback to students.
It should be accurate.
It should be helpful.
It should work quickly.
```

**SMART**:
```
### Feedback Generation (For Essays)

**Specific Requirement**:
The system generates feedback by evaluating each essay against
a teacher-defined rubric with 3-5 criteria.

**Measurable Criteria**:
- Feedback is 50-200 words per submission
- Feedback addresses at least 2 of the rubric criteria explicitly
- Feedback includes 1 specific example from the submission
- Response time is < 3 seconds per submission

**Achievable**:
Uses template-based feedback (not AI-generated) combined with
basic text analysis to identify problematic areas.

**Relevant**:
Teachers need specific, actionable feedback. Students need to know
what they did well and what to improve.

**Time-bound**:
Processing budget: 3 seconds per essay (includes API calls if any).
```

---

## Part 4: Test the Refined Criteria

**Now ask your companion to build from the SMART criteria:**

```
I want you to write a feedback function with these specifications:

### Feedback Generation (For Essays)

Specific Requirement:
The system generates feedback by evaluating each essay against
a teacher-defined rubric with 3-5 criteria.

Measurable Criteria:
- Feedback is 50-200 words per submission
- Feedback addresses at least 2 of the rubric criteria explicitly
- Feedback includes 1 specific example from the submission
- Response time is < 3 seconds per submission

Write Python code that does this. Use this sample essay:

Essay: "Climate change is important. We should do something about it.
       Trees are good. Recycling helps. We need to act now."

Rubric:
- Argument: Is there a clear thesis?
- Evidence: Are claims supported by examples?
- Clarity: Is the writing clear and organized?"
```

Your companion will produce something much better:

```python
from dataclasses import dataclass
from typing import Optional

@dataclass
class RubricCriterion:
    name: str
    description: str

def generate_feedback(
    essay: str,
    rubric: list[RubricCriterion],
    max_length: int = 200
) -> str:
    """Generate feedback evaluating essay against rubric criteria."""

    feedback_parts = []

    # Check for thesis (Argument criterion)
    if len(essay.split('.')) < 3:
        feedback_parts.append(
            "Your argument needs a clearer thesis. "
            "Currently, sentences like 'Climate change is important' "
            "state the topic but don't defend a specific position."
        )

    # Check for evidence
    if 'because' not in essay.lower() and 'example' not in essay.lower():
        feedback_parts.append(
            "Add evidence for your claims. You wrote 'Trees are good' "
            "but didn't explain why or provide examples."
        )

    # Compile feedback
    feedback = " ".join(feedback_parts)

    # Trim to word limit
    words = feedback.split()
    if len(words) > max_length:
        feedback = " ".join(words[:max_length]) + "..."

    return feedback
```

---

## Part 5: Extract Your Acceptance Criteria Template

[template content unchanged]

---

## Part 6: Build Your Grading System Acceptance Criteria

[features and SMART criteria unchanged]

---

## Part 7: Why SMART Criteria Matter

[bullets unchanged]

---

## Try With AI

[prompts unchanged]

---

## Safety & Next Steps

You've now mastered the most critical skill in specification-driven development: **writing clear, testable acceptance criteria.**

References:
- Spec: `https://github.com/panaversity/spec-kit-plus/blob/main/docs-plus/06_core_commands/03_spec/readme.md`
- Plan: `https://github.com/panaversity/spec-kit-plus/blob/main/docs-plus/06_core_commands/04_plan/readme.md`

---

## Acceptance Checklist (Lesson 2)

- [ ] Rewrote at least one feature’s criteria using SMART format
- [ ] Each criterion is objectively testable (numbers, explicit outcomes)
- [ ] Criteria avoid subjective terms (e.g., “good”, “accurate”) without definition
- [ ] Mapped criteria back to the spec’s Success Criteria
- [ ] Verified criteria can be implemented without changing unspecified tech choices

**Next**: You'll run `/sp.specify` and discover that your clear thinking has set you up to get even clearer feedback from the tool.
