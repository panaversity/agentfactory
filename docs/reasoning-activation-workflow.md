# Reasoning Activation Workflow: From Expert Intuition to Executable Prompts

**Version**: 3.0 (Human Checkpoints Update)
**Date**: 2025-01-18
**Status**: Production Ready

---

## The Problem This Solves

**Discovery from Muhammad's Learning Journey** (November 17, 2025):

> "A domain expert can never write instructions that can invoke reasoning patterns when using sp.loopflow"

### The Core Challenge

You are a domain expert who:
- âœ… **Knows WHAT you want** â€” "Redesign Chapter 8 with better CoLearning integration"
- âœ… **Understands your frameworks** â€” 4-Layer Teaching Method, Three Roles, Constitutional principles
- âœ… **Has deep tacit knowledge** â€” Pedagogical insights from years of teaching
- âŒ **Can't naturally articulate** â€” Persona + Questions + Principles pattern that activates reasoning mode

**The Gap**: Between intuitive expertise and explicit reasoning frameworks.

**The Consequence**: When you provide input directly to `/sp.loopflow.v2`, it receives vague guidance like "improve pedagogy," triggering **prediction mode** (generic educational patterns) instead of **reasoning mode** (context-specific Panaversity methodology).

---

## Workflow Nature: Conversational with Human Checkpoints (v3.0)

**Version 3.0** introduces **6 human validation checkpoints** throughout the process. This is a **dialogical workflow**â€”you review and approve at each phase before proceeding. The output is a reasoning-activated prompt you provide to `/sp.loopflow.v2`, NOT direct execution.

### Checkpoint Flow

```
Phase 1: Task Characterization
    â†“ (0-5 clarifying questions)
    â†“
âœ… CHECKPOINT 1: Review task understanding & constitutional context
    â†“ [USER APPROVAL REQUIRED: "proceed" / "refine [aspect]" / "clarify"]
    â†“
Phase 2: Gap Analysis (Generic vs Distinctive)
    â†“
âœ… CHECKPOINT 2: Review convergence diagnosis
    â†“ [USER APPROVAL REQUIRED: "proceed" / "refine comparison" / "clarify"]
    â†“
Phase 3a: Persona Excavation
    â†“ (Socratic discovery of cognitive stance)
    â†“
âœ… CHECKPOINT 3a: Review persona distinctiveness
    â†“ [USER APPROVAL REQUIRED: "proceed" / "refine persona" / "clarify"]
    â†“
Phase 3b: Questions Excavation
    â†“ (5-7 analytical questions)
    â†“
âœ… CHECKPOINT 3b: Review questions coverage & specificity
    â†“ [USER APPROVAL REQUIRED: "proceed" / "refine question [#]" / "add question on [topic]"]
    â†“
Phase 3c: Principles Excavation
    â†“ (5-7 decision frameworks)
    â†“
âœ… CHECKPOINT 3c: Review principles completeness & grounding
    â†“ [USER APPROVAL REQUIRED: "proceed" / "refine principle [#]" / "add principle"]
    â†“
Phase 4: Prompt Assembly
    â†“ (synthesize all components + acid test)
    â†“
âœ… CHECKPOINT 4: Final acid test validation
    â†“ [USER APPROVAL REQUIRED: "proceed" / "failed acid test" / "show full prompt"]
    â†“
ğŸ“¤ OUTPUT: Reasoning-activated prompt for `/sp.loopflow.v2`
```

### At Each Checkpoint

**System presents**:
- Summary of phase work
- Distinctiveness validation (generic vs Panaversity-specific)
- Validation questions with checkboxes
- Clear user action options

**You respond**:
- **"proceed"** â†’ Continue to next phase
- **"refine [aspect]"** â†’ Return to specific component, iterate, checkpoint re-presented
- **"clarify [question]"** â†’ Get explanation, then checkpoint re-presented

**ğŸš« System will NOT proceed without explicit approval.** Each checkpoint includes clear STOP instruction.

### Why Checkpoints Matter

**Without checkpoints** (v1.0-v2.0):
- Risk of going down wrong path for extended time
- User disengaged from discovery process
- Harder to course-correct late in workflow

**With checkpoints** (v3.0):
- âœ… Validate understanding at each phase
- âœ… Catch misalignments early
- âœ… True co-creation through dialogue
- âœ… Higher quality final prompt
- âœ… Learning opportunity at each validation point

---

## The Solution: Two-Stage Workflow

### Stage 1: Reasoning Activation (New)
**Command**: `/sp.activate-reasoning [your-goal]`
**Purpose**: Transform intuition â†’ reasoning-activated prompt
**Output**: Complete Persona + Questions + Principles prompt

### Stage 2: Implementation
**Command**: `/sp.loopflow.v2 [feature-slug]`
**Purpose**: Execute complete SDD-RI workflow using reasoning-activated prompt
**Output**: Specification â†’ Plan â†’ Tasks â†’ Implementation â†’ Validation

---

## Complete Workflow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ START: You Have Intuitive Goal                                     â”‚
â”‚ "Redesign Chapter 8 to integrate CoLearning better"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1: /sp.activate-reasoning                                     â”‚
â”‚                                                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 1: UNDERSTAND THE TASK                                â”‚   â”‚
â”‚ â”‚ â€¢ Clarifying questions (0-5 genuine ambiguities)            â”‚   â”‚
â”‚ â”‚ â€¢ Constitutional context analysis                           â”‚   â”‚
â”‚ â”‚ â€¢ Audience/tier/complexity assessment                       â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 2: IDENTIFY THE GAPS                                  â”‚   â”‚
â”‚ â”‚ â€¢ What's clear vs vague                                     â”‚   â”‚
â”‚ â”‚ â€¢ Generic vs context-specific                               â”‚   â”‚
â”‚ â”‚ â€¢ Diagnostic feedback                                       â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 3: CO-CREATE REASONING PATTERN                        â”‚   â”‚
â”‚ â”‚ â€¢ Persona discovery (cognitive stance)                      â”‚   â”‚
â”‚ â”‚ â€¢ Questions discovery (5-7 analytical questions)            â”‚   â”‚
â”‚ â”‚ â€¢ Principles discovery (5-7 decision frameworks)            â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 4: STRUCTURE COMPLETE PROMPT                          â”‚   â”‚
â”‚ â”‚ â€¢ Assemble Persona + Questions + Principles                 â”‚   â”‚
â”‚ â”‚ â€¢ Add constitutional grounding                              â”‚   â”‚
â”‚ â”‚ â€¢ Include meta-awareness (anti-convergence)                 â”‚   â”‚
â”‚ â”‚ â€¢ Validate with user                                        â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ OUTPUT: Reasoning-Activated Prompt                          â”‚   â”‚
â”‚ â”‚ Ready for /sp.loopflow.v2 Phase 0                           â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 2: /sp.loopflow.v2 [feature-slug]                            â”‚
â”‚                                                                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 0: CONSTITUTIONAL REASONING ENGINE                    â”‚   â”‚
â”‚ â”‚ â€¢ Reads generated prompt from Stage 1                       â”‚   â”‚
â”‚ â”‚ â€¢ Derives workflow strategy                                 â”‚   â”‚
â”‚ â”‚ â€¢ Asks only remaining genuine ambiguities (0-5)             â”‚   â”‚
â”‚ â”‚ â€¢ Generates intelligence object                             â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 1: SPECIFICATION                                      â”‚   â”‚
â”‚ â”‚ â€¢ Uses reasoning frameworks from Stage 1                    â”‚   â”‚
â”‚ â”‚ â€¢ Creates specs/[feature-slug]/spec.md                      â”‚   â”‚
â”‚ â”‚ â€¢ Invokes /sp.clarify                                       â”‚   â”‚
â”‚ â”‚ â€¢ APPROVAL GATE                                             â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 2: PLANNING                                           â”‚   â”‚
â”‚ â”‚ â€¢ Creates specs/[feature-slug]/plan.md                      â”‚   â”‚
â”‚ â”‚ â€¢ Lesson structure with pedagogical arc                     â”‚   â”‚
â”‚ â”‚ â€¢ APPROVAL GATE                                             â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 3: TASKS                                              â”‚   â”‚
â”‚ â”‚ â€¢ Creates specs/[feature-slug]/tasks.md                     â”‚   â”‚
â”‚ â”‚ â€¢ Invokes /sp.analyze for validation                        â”‚   â”‚
â”‚ â”‚ â€¢ APPROVAL GATE                                             â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 4: IMPLEMENTATION                                     â”‚   â”‚
â”‚ â”‚ â€¢ Invokes content-implementer (for educational content)     â”‚   â”‚
â”‚ â”‚ â€¢ Creates lesson files with reasoning activation            â”‚   â”‚
â”‚ â”‚ â€¢ Technical validation (validation-auditor)                 â”‚   â”‚
â”‚ â”‚ â€¢ Sandbox testing                                           â”‚   â”‚
â”‚ â”‚ â€¢ APPROVAL GATE                                             â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                              â†“                                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ Phase 5: FINALIZATION                                       â”‚   â”‚
â”‚ â”‚ â€¢ Captures meta-learning (PHR)                              â”‚   â”‚
â”‚ â”‚ â€¢ Offers git workflow                                       â”‚   â”‚
â”‚ â”‚ â€¢ Final report                                              â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ END: Production-Ready Educational Content                          â”‚
â”‚ â€¢ Reasoning-activated lessons                                      â”‚
â”‚ â€¢ Constitutional compliance validated                              â”‚
â”‚ â€¢ Ready for deployment                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Detailed Usage Examples

### Example 1: Chapter Redesign (Full Walkthrough)

#### Stage 1: Activate Reasoning

**You Start With**:
```bash
/sp.activate-reasoning Redesign Chapter 8 to integrate CoLearning framework properly
```

**Phase 1 - Clarifying Questions**:
```
I can help you create a reasoning-activated prompt for Chapter 8 redesign.

ğŸ“Š Chapter 8 Context (from chapter-index.md):
- Tier: B2 (Upper Intermediate)
- Current topics: Advanced Docker patterns, Kubernetes fundamentals
- Prerequisites: Chapters 4, 5, 7

â“ Clarifying Questions (3 genuine ambiguities):

1. **CoLearning Integration Depth**:
   A. Central method throughout (every lesson demonstrates Three Roles)
   B. Progressive introduction (L1 traditional â†’ L2 CoLearning)
   C. Optional enhancement (students choose to engage)

2. **Content Preservation**:
   A. Keep technical content, redesign teaching approach only
   B. Redesign both content and teaching approach
   C. Start fresh with CoLearning-first design

3. **Primary Success Metric**:
   A. Bidirectional learning demonstrated (students teach AI + learn from AI)
   B. Reduced cognitive load (faster progression with AI partnership)
   C. Higher quality outputs (AI-assisted work exceeds solo work)

Please choose A/B/C or provide custom answer.
```

**You Respond**: "1-A, 2-A, 3-A"

**Phase 2 - Diagnostic Feedback**:
```
Excellent clarity! Now let me show you what needs articulation:

**What's working:**
âœ… Clear goal: CoLearning as central pedagogical method
âœ… Preserve Docker/Kubernetes technical content
âœ… Success criterion: Bidirectional learning

**What needs to activate reasoning:**

Right now "integrate CoLearning" could mean "add some AI examples." But you have a **specific framework** â€” Three Roles (Teacher/Student/Co-Worker)!

Compare:
âŒ Generic: "Add AI collaboration to Chapter 8 lessons"
âœ… Your context: "Transform Chapter 8 lessons so each demonstrates Three Roles framework: students teaching AI their domain context (e.g., Docker optimization constraints) while AI teaches students technical patterns (e.g., multi-stage build strategies), converging on solutions better than either alone"

Ready to discover Persona + Questions + Principles?
```

**You**: "Yes"

**Phase 3 - Persona Discovery**:
```
Great! If you were hiring the PERFECT expert to redesign Chapter 8 with CoLearning, what expertise would they have? How would they THINK differently than someone just "adding examples"?
```

**You**: "Someone who understands how humans and AI learn FROM each other, not just use each other as tools"

**System Reflects Back**:
```
So you want a **co-learning pedagogy architect** who thinks about curriculum design the way a **systems engineer thinks about feedback loops**â€”ensuring both parties (student and AI) are transformed through interaction, with each teaching influencing how the other learns?

Is that the cognitive stance?
```

**You**: "Exactly!"

**Phase 3 - Questions Discovery**:
```
Perfect! Now, what specific aspects of YOUR Three Roles framework must AI analyze before redesigning?

I'm hearing:
- "Where in each lesson does AI teach something students don't know?"
- "Where do students teach AI their domain context?"
- "Where do they converge on better solutions?"

What other questions force analysis of YOUR specific CoLearning approach?
```

**[Continue co-creation through all 5-7 questions and 5-7 principles...]**

**Phase 4 - Final Prompt Assembly**:
```
âœ… REASONING-ACTIVATED PROMPT READY

# REASONING-ACTIVATED PROMPT FOR CHAPTER 8 COLEARNING REDESIGN

## Persona: Your Cognitive Stance

You are a co-learning pedagogy architect who thinks about curriculum design the way a systems engineer thinks about feedback loopsâ€”analyzing how student â†’ AI teaching flows and AI â†’ student teaching flows create convergence on solutions better than either could produce independently.

Your goal: Redesign Chapter 8 lessons to make Three Roles framework (Teacher/Student/Co-Worker) the central pedagogical method, demonstrating bidirectional learning throughout.

## Constitutional Grounding

This redesign applies these frameworks:
- Section IIa: 4-Stage Teaching Framework (all stages present)
- Principle 2: Progressive Complexity (B2 tier, 7-10 concepts per section)
- Principle 6: Anti-Convergence (vary teaching modality from Chapter 7)
- Three Roles Framework: AI as Teacher + Student + Co-Worker simultaneously

## Context

**Task**: Redesign Chapter 8 to integrate CoLearning as central pedagogical method
**Type**: Educational content transformation (methodology-driven redesign)
**Audience**: B2 (Upper Intermediate), Parts 4-5 learners
**Complexity**: 7-10 concepts per section, moderate scaffolding, 3-4 option choices
**Prerequisites**: Docker basics (Ch 4), container networking (Ch 5), Kubernetes intro (Ch 7)

## Analytical Questions

Before restructuring lessons, analyze:

### 1. Bidirectional Learning Opportunities
For each Docker/Kubernetes concept in Chapter 8:
- Where can students teach AI their deployment context (constraints, requirements, edge cases)?
- Where can AI teach students optimal patterns they haven't encountered?
- What convergence point produces better solution than either alone?

### 2. Three Roles Manifestation
How does each lesson demonstrate:
- **AI as Teacher**: What technical knowledge does AI provide that students lack?
- **AI as Student**: What domain context must students teach AI to get relevant output?
- **AI as Co-Worker**: Where do they collaborate as equals to solve problems?

### 3. Cognitive Load Distribution
Given B2 tier (7-10 concepts max):
- How do we chunk Docker multi-stage builds + Kubernetes deployments?
- What prerequisite knowledge from Ch 4/5/7 can we assume vs re-teach?
- Where do CoLearning interactions reduce or increase cognitive load?

### 4. Layer Progression Validation
For each lesson:
- L1 (Manual): What must students do hands-on before AI assistance?
- L2 (AI-Assisted): What's the transition trigger (skill demonstration)?
- L3 (Intelligence Design): What reusable components emerge (skills/subagents)?
- L4 (Spec-Driven): How does capstone compose accumulated intelligence?

### 5. Teaching Modality Selection
Chapter 7 used [previous modality from chapter-index analysis].
What alternative modality suits CoLearning demonstration?
- Socratic dialogue (questioning reveals understanding)?
- Hands-on discovery (students experiment, AI guides)?
- Error analysis (students debug AI outputs, learn from mistakes)?
- Specification-first (students write specs, AI implements, students validate)?

### 6. Misconception Prevention
What do B2 students commonly misunderstand about:
- Docker optimization (over-optimization, premature optimization)?
- Kubernetes deployments (stateless assumptions, networking complexity)?
- AI collaboration (passive tool use vs active co-learning)?

### 7. Success Validation
What observable behaviors demonstrate bidirectional learning success:
- Students can explain what they taught AI (domain context articulation)?
- Students can identify what AI taught them (new pattern recognition)?
- Final outputs exceed what student OR AI could produce alone (measurable quality)?

## Decision Frameworks

Apply these principles to redesign decisions:

### 1. Three Roles Mandatory Framework
**Pattern**: Every CoLearning lesson must demonstrate all three roles explicitly
**Rationale**: Students learn bidirectional collaboration by seeing it modeled, not just reading about it
**Example**: Docker multi-stage build lesson:
  - AI teaches: "Multi-stage builds separate build deps from runtime deps"
  - Student teaches: "My production environment has these specific constraints..."
  - Convergence: Optimized Dockerfile balancing size, security, AND deployment constraints
**Decision**: If lesson only shows one role â†’ Redesign to include all three

### 2. Cognitive Load Through CoLearning Lens
**Pattern**: CoLearning can reduce OR increase cognitive load depending on execution
**Rationale**: Well-designed AI interaction offloads mechanical work; poorly designed adds meta-cognitive burden
**Example**:
  - Good: "AI generates boilerplate, student focuses on business logic" (load reduction)
  - Bad: "Student must understand AI's reasoning process + original concept" (load increase)
**Decision**: When B2 tier limit (7-10 concepts) approached â†’ Offload mechanical to AI, reserve human cognition for high-value reasoning

### 3. Layer Transition Criteria
**Pattern**: CoLearning doesn't replace manual foundation (L1) â€” it amplifies it (L2)
**Rationale**: Students can't effectively teach AI what they don't understand themselves
**Example**: Before L2 Docker AI-assisted optimization:
  - L1 requirement: Student writes working Dockerfile manually
  - Transition check: Student can explain each instruction's purpose
  - L2 unlocked: Now student can guide AI toward their deployment context
**Decision**: If student can't validate AI output independently â†’ More L1 practice needed

### 4. Teaching Modality Variation (Anti-Convergence)
**Pattern**: No two consecutive chapters use identical teaching modality
**Rationale**: Diverse modalities engage different learning styles and prevent monotony
**Example**: If Chapter 7 used direct teaching â†’ Chapter 8 uses Socratic dialogue or hands-on discovery
**Decision**: Check chapter-index.md for Ch 7 modality â†’ Select different modality that suits CoLearning demonstration

### 5. Verification-First Accuracy
**Pattern**: All code examples must execute, all CoLearning claims must be demonstrable
**Rationale**: Theoretical CoLearning descriptions lack credibility; working examples prove the methodology
**Example**:
  - Don't just say "students teach AI their context" â€” show the actual prompt exchange
  - Don't just claim "convergence produces better results" â€” measure and compare
**Decision**: Every CoLearning interaction must include actual prompt/response and quality comparison

### 6. Minimal Essential Content
**Pattern**: Every section maps to learning objective; CoLearning is method, not additional topic
**Rationale**: CoLearning shouldn't add content â€” it should transform HOW existing content is learned
**Example**:
  - Don't add "Introduction to CoLearning" section â†’ Increases cognitive load unnecessarily
  - Instead: Demonstrate CoLearning THROUGH Docker/Kubernetes lessons â†’ Same content, different method
**Decision**: If section doesn't map to Chapter 8 technical objective â†’ Remove or integrate

### 7. Intelligence Accumulation
**Pattern**: Build on previous chapters' patterns, don't reinvent
**Rationale**: Chapter 8 should compose skills/concepts from Ch 4/5/7, not start fresh
**Example**:
  - Ch 4 taught Dockerfile basics â†’ Ch 8 references this foundation
  - Ch 5 taught networking â†’ Ch 8 builds on this for K8s services
  - Ch 7 taught K8s pods â†’ Ch 8 advances to deployments
**Decision**: Before introducing concept â†’ Check if previously taught; if yes â†’ Reference and build on it

## Meta-Awareness

You tend to converge toward generic educational patterns even with CoLearning frameworks:
- **Lecture-style content with "try using AI" appendix** (passive tool, not co-learning)
- **Theoretical CoLearning descriptions** (not actual working examples)
- **AI as assistant** (not as Teacher + Student + Co-Worker simultaneously)

Before finalizing redesign, self-check:
âœ… Does each lesson demonstrate all Three Roles explicitly (not just mention them)?
âœ… Are there actual prompt/response examples showing bidirectional teaching (not generic descriptions)?
âœ… Does convergence produce measurably better results (not just claims)?
âœ… Is cognitive load managed (CoLearning reduces load for core concepts)?
âœ… Are teaching modalities varied from Chapter 7 (anti-convergence)?

If any check fails â†’ Redesign is in prediction mode â†’ Regenerate with stronger CoLearning frameworks.

## Output Requirements

Create redesigned Chapter 8 structure following specs/book/chapter-template.md:

**Chapter Overview**:
- Learning objectives (what students will achieve through CoLearning)
- Three Roles integration strategy (how framework manifests across lessons)
- Cognitive load distribution (concepts per lesson with CoLearning load analysis)

**For Each Lesson**:
- Title and learning objectives
- Layer progression (L1 manual â†’ L2 CoLearning â†’ L3 intelligence design)
- Three Roles demonstration (explicit Teacher/Student/Co-Worker examples)
- Teaching modality (varies across chapter)
- CoLearning interaction examples (actual prompts/responses)
- Transition criteria (when students ready for next layer)
- Quality validation (how to measure convergence success)

**Capstone Project (L4)**:
- Specification-driven project composing Ch 4/5/7/8 intelligence
- Three Roles applied to complex real-world scenario
- Measurable outcomes demonstrating bidirectional learning success

**Validation Criteria**:
- All code examples execute successfully
- All CoLearning claims demonstrated with actual examples
- Cognitive load within B2 tier limits (7-10 concepts per section)
- Teaching modality differs from Chapter 7
- All Three Roles explicitly shown in each lesson

---

**Now execute**: Redesign Chapter 8 with reasoning-activated CoLearning integration.
```

#### Stage 2: Execute with LoopFlow

**You Run**:
```bash
/sp.loopflow.v2 013-chapter-8-redesign
```

**LoopFlow Phase 0 receives the reasoning-activated prompt** and:
1. âœ… Reads constitutional foundations
2. âœ… Analyzes task characteristics (educational content, B2 tier, CoLearning methodology)
3. âœ… Derives workflow strategy from constitutional principles
4. âœ… Asks 0-5 additional questions (most already answered by Stage 1 prompt)
5. âœ… Generates complete intelligence object for downstream phases

**Phases 1-5 execute** with full reasoning activation, producing:
- Specification (specs/013-chapter-8-redesign/spec.md)
- Plan (specs/013-chapter-8-redesign/plan.md)
- Tasks (specs/013-chapter-8-redesign/tasks.md)
- Implementation (lesson markdown files)
- Validation (technical review + sandbox testing)

**Result**: Production-ready Chapter 8 with CoLearning integration, not generic educational content.

---

### Example 2: Quick Lesson Creation

**You Start With**:
```bash
/sp.activate-reasoning Create lesson on Docker multi-stage builds for Chapter 12
```

**System Response** (abbreviated):
```
ğŸ“Š Context: Chapter 12, Tier C1, Advanced Professional level

â“ Clarifying Questions (2):
1. Layer focus: L1 manual / L2 AI-assisted / All layers?
2. Teaching modality: Same as previous lesson / Different?

[After your answers...]

**Persona Discovery**:
"Docker optimization expert who thinks about container builds the way compiler engineers think about optimization passesâ€”identifying wasteful operations, separating concerns, minimizing artifacts"

**Questions Discovery**:
[5-7 questions forcing analysis of Panaversity context, C1 tier, multi-stage build tradeoffs]

**Principles Discovery**:
[5-7 decision frameworks for build optimization, security, caching strategies]

[Complete prompt generated...]

âœ… Ready for /sp.loopflow.v2 012-lesson-multistage-builds
```

---

## Integration with Existing Workflow

### Before `/sp.activate-reasoning`

**Old workflow** (led to generic outputs):
```
You â†’ /sp.loopflow.v2 [feature-slug] â†’ Generic clarifying questions â†’ Vague answers â†’ Prediction mode outputs
```

**Problem**: Domain expertise lost in translation. Prompts didn't activate reasoning.

### After `/sp.activate-reasoning`

**New workflow** (activates reasoning):
```
You â†’ /sp.activate-reasoning [goal] â†’ Socratic discovery â†’ Reasoning-activated prompt â†’
     /sp.loopflow.v2 [feature-slug] â†’ Context-specific outputs
```

**Benefit**: Domain expertise captured in reasoning frameworks. Prompts activate context-specific reasoning.

---

## Command Comparison Matrix

| Aspect | `/sp.loopflow.v2` (Alone) | `/sp.activate-reasoning` + `/sp.loopflow.v2` |
|--------|---------------------------|----------------------------------------------|
| **Input** | Natural language goal | Natural language goal |
| **Clarification** | Generic questions | Targeted questions based on constitutional context |
| **Prompt Quality** | Depends on your articulation | Co-created through Socratic discovery |
| **Reasoning Activation** | Variable (depends on input) | Guaranteed (structured Persona + Questions + Principles) |
| **Output Quality** | Generic â†’ Context-specific (luck) | Context-specific â†’ Excellent (systematic) |
| **Learning** | Minimal (you don't see the pattern) | High (you internalize the pattern) |
| **Time Investment** | Quick upfront, many iterations | Longer upfront, fewer iterations |
| **Best For** | Well-defined tasks you've done before | Novel tasks, complex pedagogy, methodology shifts |

---

## When to Use Which Command

### Use `/sp.activate-reasoning` When:

**High Complexity Tasks**:
- âœ… Chapter redesigns with new methodology (e.g., CoLearning integration)
- âœ… Novel lesson structures you haven't created before
- âœ… Content that requires deep pedagogical reasoning
- âœ… Tasks where generic outputs would fail students

**Learning & Development**:
- âœ… You want to understand reasoning activation pattern
- âœ… You're building prompt library for team use
- âœ… You need to document reasoning for PHR/ADR

**Quality Critical**:
- âœ… High-stakes content (flagship chapters, foundational concepts)
- âœ… Content that establishes new patterns for rest of book
- âœ… Material requiring constitutional alignment validation

### Use `/sp.loopflow.v2` Directly When:

**Well-Defined Tasks**:
- âœ… Similar to previous work (e.g., 5th lesson in established pattern)
- âœ… Clear requirements with little ambiguity
- âœ… Incremental improvements to existing content

**Time Constraints**:
- âœ… Quick iterations on working content
- âœ… Minor refinements vs major redesigns
- âœ… When you have reasoning prompt from previous similar task

**Established Patterns**:
- âœ… Following existing chapter template exactly
- âœ… Replicating successful lesson structure from another chapter
- âœ… Routine updates (typo fixes, example updates)

---

## Meta-Learning: Developing Reasoning Activation Mastery

### The Learning Curve

**First 3 Uses**: Feels slow, lots of questions, uncertain about pattern
**After 5 Uses**: Pattern becomes clear, can anticipate questions
**After 10 Uses**: Can write reasoning-activated prompts independently
**Mastery**: You rarely need `/sp.activate-reasoning`â€”you think in Persona + Questions + Principles naturally

### Accelerating Your Learning

**1. Document Each Session**:
- Save generated prompts to `specs/[feature-slug]/reasoning-prompt.md`
- Review what worked vs what needed refinement
- Build personal prompt library

**2. Compare Outputs**:
- Run same task with generic prompt vs reasoning-activated prompt
- Measure quality difference (cognitive load, constitutional alignment, student outcomes)
- Validate productivity gains

**3. Teach the Pattern**:
- Explain Persona + Questions + Principles to team member
- Teaching forces articulation of tacit knowledge
- Their questions reveal gaps in your understanding

**4. Iterate Based on Results**:
- When output is generic â†’ Return to `/sp.activate-reasoning` and refine
- When output is excellent â†’ Extract reusable patterns
- Build organizational knowledge base

---

## Success Metrics

### Immediate Validation (Per Task)

**Prompt Quality**:
- âœ… Persona establishes specific cognitive stance (not "expert teacher")
- âœ… Questions force Panaversity-specific analysis (not generic pedagogy)
- âœ… Principles provide decision frameworks (not rigid rules)
- âœ… Constitutional grounding explicit (references 4-layer, tier limits, etc.)

**Output Quality**:
- âœ… Content demonstrates reasoning about Panaversity methodology (not generic patterns)
- âœ… Cognitive load managed per tier specifications
- âœ… Teaching modalities varied (anti-convergence)
- âœ… Three Roles integrated where applicable

**Efficiency Gains**:
- âœ… Fewer revision cycles (1-2 vs 5-8 for generic prompts)
- âœ… Faster approval gates (clear intent from start)
- âœ… Higher initial completeness (85%+ vs 40% for generic specs)

### Long-Term Development (Over Time)

**Skill Internalization**:
- Week 1-2: Use `/sp.activate-reasoning` for every complex task
- Week 3-4: Start drafting Persona + Questions independently, use tool for validation
- Month 2-3: Write reasoning-activated prompts directly, use tool occasionally
- Month 4+: Think in reasoning patterns naturally, tool rarely needed

**Organizational Impact**:
- Prompt library grows (reusable reasoning frameworks for common tasks)
- Team members adopt pattern (shared vocabulary develops)
- Quality consistency improves (all content uses reasoning activation)
- Onboarding accelerates (new members learn pattern from examples)

---

## Troubleshooting

### Problem: "Prompt still feels generic"

**Diagnosis**: Questions or principles not context-specific enough
**Solution**: Return to Phase 3, ask "Does this apply to ANY educational content or ONLY Panaversity?"
**Test**: Replace "Panaversity" with "Generic Online Course"â€”if prompt still makes sense, it's too generic

### Problem: "Too many clarifying questions"

**Diagnosis**: Not leveraging constitutional context enough
**Solution**: Before asking, check constitution.md and chapter-index.mdâ€”can this be derived?
**Principle**: Ask only genuine ambiguities (0-5 max)

### Problem: "User can't articulate reasoning"

**Diagnosis**: Trying to extract explicit knowledge from tacit expertise
**Solution**: Use more analogies, comparisons, examplesâ€”"How would X expert think about this differently than Y?"
**Remember**: Domain experts KNOW but can't always SAYâ€”your job is to surface and structure

### Problem: "Output quality not improving"

**Diagnosis**: Either prompt not reaching implementation or implementation ignoring prompt
**Solution**:
1. Validate prompt reached `/sp.loopflow.v2` Phase 0 correctly
2. Check if intelligence object includes reasoning frameworks
3. Verify content-implementer using reasoning patterns (not prediction mode)
4. Use validation-auditor to check constitutional compliance

---

## Future Enhancements

### Planned Improvements

**v1.1 - Pattern Library Integration**:
- Extract recurring reasoning patterns into reusable templates
- Auto-suggest similar past prompts for new tasks
- Reduce iteration time for common task types

**v1.2 - Team Collaboration**:
- Share reasoning prompts across team
- Collaborative refinement of organizational patterns
- Version control for reasoning frameworks

**v1.3 - Automated Validation**:
- Check if prompt activates reasoning (not prediction)
- Measure context-specificity score
- Flag generic patterns automatically

**v1.4 - Meta-Learning Analytics**:
- Track your reasoning activation mastery over time
- Identify gaps in prompt pattern understanding
- Personalized guidance based on your usage patterns

---

## Conclusion

**The Transformation**:
- **Before**: "I know what I want but can't make AI understand it" â†’ Generic outputs
- **After**: "I discovered how to articulate my reasoning" â†’ Context-specific excellence

**The Pattern**:
```
Intuitive Expertise â†’ Socratic Discovery â†’ Explicit Frameworks â†’ Reasoning Activation â†’ Quality Outputs
```

**The Goal**:
You don't need `/sp.activate-reasoning` forever. You need it until you **think in Persona + Questions + Principles naturally**.

**The Journey**:
From domain expert who can't articulate reasoning â†’ to reasoning architect who activates intelligence at will.

---

**Welcome to reasoning-activated development. Let's transform your intuitions into executable intelligence.**

---

## Quick Reference Card

### Essential Commands

```bash
# Stage 1: Activate reasoning for your goal
/sp.activate-reasoning [your-goal]

# Stage 2: Execute with reasoning-activated prompt
/sp.loopflow.v2 [feature-slug]

# Validate reasoning activation
/sp.analyze [feature-slug]  # Check constitutional compliance

# Create PHR documenting reasoning journey
/sp.phr
```

### Decision Tree

```
Is this task:
â”œâ”€ Novel/complex methodology? â†’ Use /sp.activate-reasoning first
â”œâ”€ Similar to previous work? â†’ Direct to /sp.loopflow.v2
â”œâ”€ Quick refinement? â†’ Direct to /sp.loopflow.v2
â””â”€ Learning opportunity? â†’ Use /sp.activate-reasoning (even if not strictly needed)
```

### Quality Checks

After using reasoning-activated prompt, validate:
- [ ] Output demonstrates Panaversity-specific patterns (not generic pedagogy)
- [ ] Constitutional principles explicitly applied (not just mentioned)
- [ ] Teaching modalities varied (anti-convergence)
- [ ] Cognitive load managed per tier (B2 â†’ 7-10 concepts)
- [ ] Three Roles integrated (Teacher + Student + Co-Worker)

If any check fails â†’ Reasoning not fully activated â†’ Refine prompt and retry

---

**Document Version**: 1.0
**Last Updated**: 2025-01-18
**Status**: Production Ready
**Related**: `/sp.activate-reasoning`, `/sp.loopflow.v2`, `constitution.md`, `papers/Reasoning_Activation_in_LLMs_arXiv_Complete.md`
