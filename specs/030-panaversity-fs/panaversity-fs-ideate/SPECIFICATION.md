# PanaversityFS: Agent-Native Multi-Book Storage System

**Version**: 1.0.0
**Status**: Implementation Ready
**Timeline**: 3 Weeks (Phased Rollout)
**Architecture**: OpenDAL (Storage) + AgentFS (Audit) + Cloudflare MCP (Patterns)

---

## Executive Summary

**PanaversityFS** is a production-grade, agent-native storage system designed for managing multiple educational books with built-in vertical intelligence. It combines three best-of-breed systems:

1. **Apache OpenDAL**: Universal storage abstraction (Local/R2/Supabase/S3)
2. **AgentFS Patterns**: Audit trails, agent tracking, observability
3. **Cloudflare MCP Architecture**: OAuth, metrics, error handling, tool patterns

### What Problem Does This Solve?

**Current State (Git-based)**:

- âœ… 84 chapters in `apps/learn-app/docs/`
- âœ… Assets in `book-source/static/`
- âŒ Single book only (not extensible)
- âŒ No agent-native operations (agents must clone entire repo)
- âŒ No audit trail (who changed what?)
- âŒ No summaries (manual work)
- âŒ No observability (can't see book operations)
- âŒ Tightly coupled to filesystem

**Future State (PanaversityFS)**:

- âœ… Multi-book registry (unlimited books)
- âœ… Agent-native MCP interface (13 specialized tools)
- âœ… Complete audit trail (every operation logged)
- âœ… Auto-generated summaries (AI-powered)
- âœ… Full observability (metrics dashboard)
- âœ… Storage abstraction (Local/R2/Supabase)
- âœ… Vertical Intelligence (LanceDB vectors, content validation)

### Key Capabilities

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PANAVERSITYFS CAPABILITIES                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  ðŸ“š Multi-Book Management                                    â”‚
â”‚     â€¢ Registry of all books                                  â”‚
â”‚     â€¢ Per-book configurations                                â”‚
â”‚     â€¢ Cross-book operations                                  â”‚
â”‚                                                              â”‚
â”‚  ðŸ¤– Agent-Native Operations (13 MCP Tools)                   â”‚
â”‚     â€¢ Content: add, update, read, delete                     â”‚
â”‚     â€¢ Assets: upload, download, list (video/audio/slides)   â”‚
â”‚     â€¢ Summaries: generate, update, query                     â”‚
â”‚     â€¢ Discovery: list books, search, structure               â”‚
â”‚                                                              â”‚
â”‚  ðŸ“Š Complete Observability                                   â”‚
â”‚     â€¢ Audit logs (JSONL, queryable)                          â”‚
â”‚     â€¢ Metrics (Cloudflare Analytics Engine)                  â”‚
â”‚     â€¢ Agent tracking (who did what, when)                    â”‚
â”‚     â€¢ Error tracking (Sentry integration ready)              â”‚
â”‚                                                              â”‚
â”‚  ðŸ§  Vertical Intelligence                                    â”‚
â”‚     â€¢ Auto-summary generation (Claude API)                   â”‚
â”‚     â€¢ Vector search (LanceDB integration)                    â”‚
â”‚     â€¢ Content validation (consistency checks)                â”‚
â”‚     â€¢ Asset optimization (image/video processing)            â”‚
â”‚                                                              â”‚
â”‚  ðŸ”Œ Storage Abstraction (OpenDAL)                            â”‚
â”‚     â€¢ Local filesystem (development)                         â”‚
â”‚     â€¢ Cloudflare R2 (production)                             â”‚
â”‚     â€¢ Supabase Storage (alternative)                         â”‚
â”‚     â€¢ AWS S3 (backup)                                        â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Table of Contents

**Part 1: Architecture**

- [1. System Architecture](#1-system-architecture)
- [2. Storage Layer (OpenDAL)](#2-storage-layer-opendal)
- [3. Audit Layer (AgentFS Patterns)](#3-audit-layer-agentfs-patterns)
- [4. MCP Layer (Cloudflare Patterns)](#4-mcp-layer-cloudflare-patterns)
- [5. Intelligence Layer (LanceDB)](#5-intelligence-layer-lancedb)

**Part 2: Data Models**

- [6. Multi-Book Registry](#6-multi-book-registry)
- [7. Book Schema](#7-book-schema)
- [8. Content Structure](#8-content-structure)
- [9. Audit Log Format](#9-audit-log-format)

**Part 3: MCP Tools (Agent Interface)**

- [10. Content Operations (4 tools)](#10-content-operations)
- [11. Asset Operations (3 tools)](#11-asset-operations)
- [12. Summary Operations (3 tools)](#12-summary-operations)
- [13. Discovery & Search (3 tools)](#13-discovery-search)

**Part 4: Implementation**

- [14. Technology Stack](#14-technology-stack)
- [15. File Structure](#15-file-structure)
- [16. Core Components](#16-core-components)
- [17. Migration Strategy](#17-migration-strategy)

**Part 5: Operations**

- [18. Deployment](#18-deployment)
- [19. Monitoring & Alerts](#19-monitoring-alerts)
- [20. Testing Strategy](#20-testing-strategy)
- [21. Security](#21-security)

**Part 6: Integration**

- [22. Docusaurus Integration](#22-docusaurus-integration)
- [23. Claude Code Integration](#23-claude-code-integration)
- [24. CI/CD Pipeline](#24-cicd-pipeline)

**Part 7: Roadmap**

- [25. Phase 1: Foundation (Week 1)](#25-phase-1-foundation)
- [26. Phase 2: Intelligence (Week 2)](#26-phase-2-intelligence)
- [27. Phase 3: Production (Week 3)](#27-phase-3-production)
- [28. Future Enhancements](#28-future-enhancements)

---

## 1. System Architecture

### 1.1 High-Level Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CONSUMERS                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    AI Agents       â”‚    Docusaurus      â”‚    Admin CLI       â”‚
â”‚  (Claude, GPT)     â”‚   (Build Time)     â”‚  (Operations)      â”‚
â”‚                    â”‚                    â”‚                    â”‚
â”‚  â€¢ Read/Write      â”‚  â€¢ Read Only       â”‚  â€¢ Migrations      â”‚
â”‚  â€¢ Generate        â”‚  â€¢ Hydrate         â”‚  â€¢ Maintenance     â”‚
â”‚  â€¢ Summarize       â”‚  â€¢ Render          â”‚  â€¢ Backups         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                      â”‚                    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MCP SERVER (13 Tools + OAuth)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Content Tools:  add, update, read, delete             â”‚  â”‚
â”‚  â”‚  Asset Tools:    upload, download, list                â”‚  â”‚
â”‚  â”‚  Summary Tools:  generate, update, query               â”‚  â”‚
â”‚  â”‚  Discovery:      list_books, search, structure         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Observability:  Metrics + Audit + Error Tracking      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      PANAVERSITYFS CORE                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Registry Manager:     Multi-book coordination         â”‚  â”‚
â”‚  â”‚  Content Manager:      CRUD operations                 â”‚  â”‚
â”‚  â”‚  Asset Manager:        Binary file handling            â”‚  â”‚
â”‚  â”‚  Summary Manager:      AI generation + caching         â”‚  â”‚
â”‚  â”‚  Audit Manager:        Operation logging               â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    STORAGE LAYER (OpenDAL)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚   Local FS   â”‚  â”‚  R2 Storage  â”‚  â”‚  Supabase    â”‚       â”‚
â”‚  â”‚  (Dev Mode)  â”‚  â”‚  (Production)â”‚  â”‚ (Alternative)â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   INTELLIGENCE LAYER                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  LanceDB:          Vector storage for RAG              â”‚  â”‚
â”‚  â”‚  Summary Engine:   Claude API integration              â”‚  â”‚
â”‚  â”‚  Validator:        Content consistency checks          â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 Design Principles

**1. Storage Agnostic**

- Use OpenDAL for universal storage interface
- Single codebase works across Local/R2/Supabase/S3
- Switch backends via configuration only

**2. Agent-Native**

- MCP as primary interface (not REST/GraphQL)
- Tools designed for LLM function calling
- Rich JSON schemas for type safety

**3. Audit Everything**

- Every operation logged (AgentFS pattern)
- Agent tracking (who did what)
- Queryable audit trail (JSONL format)

**4. Observable by Default**

- Metrics via Cloudflare Analytics Engine
- Error tracking ready (Sentry hooks)
- Performance monitoring built-in

**5. Intelligence Embedded**

- Auto-summary generation
- Vector search for RAG
- Content validation agents

**6. Multi-Book Ready**

- Central registry
- Per-book configuration
- Cross-book operations

---

## 2. Storage Layer (OpenDAL)

### 2.1 Why OpenDAL?

**Apache OpenDAL** provides "One Layer, All Storage" - a unified API across 60+ storage services.

**Key Benefits**:

- âœ… Battle-tested (Apache project, 4.6k stars)
- âœ… Language bindings (Rust, Python, Node.js, Java)
- âœ… Comprehensive backends (S3, R2, Azure, GCS, Local, etc.)
- âœ… Advanced features (streaming, caching, retries)
- âœ… Zero vendor lock-in

### 2.2 Architecture

```python
# OpenDAL provides unified API
from opendal import Operator

# Local filesystem (development)
op_local = Operator("fs", root="./content")

# Cloudflare R2 (production)
op_r2 = Operator("s3",
    bucket="panaversity-books",
    region="auto",
    endpoint="https://xxx.r2.cloudflarestorage.com",
    access_key_id="...",
    secret_access_key="..."
)

# Supabase Storage (alternative)
op_supabase = Operator("s3",  # Supabase uses S3-compatible API
    bucket="books",
    endpoint="https://xxx.supabase.co/storage/v1/s3",
    access_key_id="...",
    secret_access_key="..."
)

# All backends support same operations
content = op.read("books/ai-native-dev/lessons/01-intro.md")
op.write("books/ai-native-dev/lessons/02-new.md", content)
files = op.list("books/ai-native-dev/lessons/")
```

### 2.3 Storage Structure

```
panaversity-storage/
â”œâ”€â”€ registry.yaml                         # Master registry
â”‚
â”œâ”€â”€ .audit/                               # Global audit logs
â”‚   â”œâ”€â”€ 2025-11-24.jsonl
â”‚   â”œâ”€â”€ 2025-11-23.jsonl
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ .metrics/                             # Metrics snapshots (optional)
â”‚   â””â”€â”€ 2025-11.json
â”‚
â”œâ”€â”€ books/
â”‚   â”œâ”€â”€ ai-native-software-development/
â”‚   â”‚   â”œâ”€â”€ book.yaml                     # Book specification
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ parts/
â”‚   â”‚   â”‚   â”œâ”€â”€ 01-Introducing-AI-Driven-Development/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README.md             # Part overview
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ summary.md            # AI-generated
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ metadata.json         # Timestamps, hashes
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ chapters/
â”‚   â”‚   â”‚   â”œâ”€â”€ 01-ai-development-revolution/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ README.md
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ summary.md
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ lessons/
â”‚   â”‚   â”‚   â”œâ”€â”€ 01-moment_that_changed_everything.md
â”‚   â”‚   â”‚   â”œâ”€â”€ 01-moment_that_changed_everything.summary.md
â”‚   â”‚   â”‚   â”œâ”€â”€ 02-three-trillion-developer-economy.md
â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ assets/
â”‚   â”‚       â”œâ”€â”€ slides/
â”‚   â”‚       â”‚   â”œâ”€â”€ chapter-01-slides.pdf
â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚       â”œâ”€â”€ images/
â”‚   â”‚       â”‚   â”œâ”€â”€ diagrams/
â”‚   â”‚       â”‚   â””â”€â”€ screenshots/
â”‚   â”‚       â”œâ”€â”€ videos/
â”‚   â”‚       â”‚   â””â”€â”€ chapter-01-intro.mp4
â”‚   â”‚       â””â”€â”€ audio/
â”‚   â”‚           â””â”€â”€ chapter-01-podcast.mp3
â”‚   â”‚
â”‚   â”œâ”€â”€ python-fundamentals/              # Another book
â”‚   â”‚   â”œâ”€â”€ book.yaml
â”‚   â”‚   â”œâ”€â”€ parts/
â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚
â”‚   â””â”€â”€ typescript-mastery/               # Another book
â”‚       â””â”€â”€ ...
â”‚
â””â”€â”€ .vectors/                             # LanceDB vector storage
    â”œâ”€â”€ ai-native-software-development.lance/
    â”œâ”€â”€ python-fundamentals.lance/
    â””â”€â”€ ...
```

### 2.4 OpenDAL Python API Usage

```python
import opendal
import asyncio
from typing import List, Optional

class StorageBackend:
    """Universal storage backend using OpenDAL"""

    def __init__(self, backend_type: str, **config):
        """
        Initialize storage backend

        Args:
            backend_type: "fs", "s3", etc.
            **config: Backend-specific configuration
        """
        self.op = opendal.Operator(backend_type, **config)

    # Synchronous operations
    def read(self, path: str) -> bytes:
        """Read file content"""
        return self.op.read(path)

    def write(self, path: str, content: bytes) -> None:
        """Write file content"""
        self.op.write(path, content)

    def delete(self, path: str) -> None:
        """Delete file"""
        self.op.delete(path)

    def list(self, prefix: str = "") -> List[str]:
        """List files under prefix"""
        return [entry.path for entry in self.op.list(prefix)]

    def exists(self, path: str) -> bool:
        """Check if file exists"""
        try:
            self.op.stat(path)
            return True
        except:
            return False

    def stat(self, path: str) -> dict:
        """Get file metadata"""
        metadata = self.op.stat(path)
        return {
            "size": metadata.content_length,
            "last_modified": metadata.last_modified,
            "etag": metadata.etag
        }

    # Streaming operations (for large files)
    def read_stream(self, path: str, offset: int = 0, size: Optional[int] = None):
        """Read file in chunks (for large files)"""
        reader = self.op.reader(path)
        if size is None:
            size = self.stat(path)["size"]

        chunk_size = 1024 * 1024  # 1MB chunks
        for i in range(offset, offset + size, chunk_size):
            chunk_end = min(i + chunk_size, offset + size)
            yield reader.read(i, chunk_end - i)

    def write_stream(self, path: str, stream):
        """Write file from stream (for large files)"""
        writer = self.op.writer(path)
        for chunk in stream:
            writer.write(chunk)
        writer.close()
```

### 2.5 Configuration System

```python
# config/storage.yaml
storage:
  backend: "r2"  # or "local", "supabase"

  backends:
    local:
      type: "fs"
      root: "./content"

    r2:
      type: "s3"
      bucket: "panaversity-books"
      region: "auto"
      endpoint: "${R2_ENDPOINT}"
      access_key_id: "${R2_ACCESS_KEY_ID}"
      secret_access_key: "${R2_SECRET_ACCESS_KEY}"

    supabase:
      type: "s3"  # Supabase Storage is S3-compatible
      bucket: "books"
      region: "us-east-1"
      endpoint: "${SUPABASE_ENDPOINT}/storage/v1/s3"
      access_key_id: "${SUPABASE_ACCESS_KEY_ID}"
      secret_access_key: "${SUPABASE_SECRET_ACCESS_KEY}"

    aws_s3:
      type: "s3"
      bucket: "panaversity-books-backup"
      region: "us-east-1"
      access_key_id: "${AWS_ACCESS_KEY_ID}"
      secret_access_key: "${AWS_SECRET_ACCESS_KEY}"

# Load configuration
import yaml
import os

def load_storage_config():
    with open("config/storage.yaml") as f:
        config = yaml.safe_load(f)

    # Substitute environment variables
    backend_name = config["storage"]["backend"]
    backend_config = config["storage"]["backends"][backend_name]

    resolved_config = {}
    for key, value in backend_config.items():
        if isinstance(value, str) and value.startswith("${") and value.endswith("}"):
            env_var = value[2:-1]
            resolved_config[key] = os.getenv(env_var)
        else:
            resolved_config[key] = value

    return backend_config["type"], resolved_config

# Usage
backend_type, config = load_storage_config()
storage = StorageBackend(backend_type, **config)
```

---

## 3. Audit Layer (AgentFS Patterns)

### 3.1 Why Audit Everything?

**AgentFS** (by Turso) pioneered agent-native filesystem patterns with complete audit trails.

**Key Insights**:

- Track every operation (who, what, when)
- Enable debugging (what changed?)
- Support compliance (audit history)
- Enable rollback (undo changes)

### 3.2 Audit Log Format

**Storage**: JSONL (JSON Lines) - one JSON object per line, append-only

**Benefits**:

- Streaming-friendly (process line-by-line)
- Append-only (no corruption risk)
- Human-readable (JSON)
- Query-friendly (grep, jq, etc.)

### 3.3 Audit Entry Schema

```python
from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Optional, Dict, Any
import json

@dataclass
class AuditEntry:
    """Single audit log entry"""

    # Required fields
    timestamp: str              # ISO 8601 UTC timestamp
    operation: str              # Operation type
    path: str                   # Resource path
    agent_id: str               # Agent identifier

    # Optional fields
    book_id: Optional[str] = None           # Book being operated on
    user_id: Optional[str] = None           # Human user (if applicable)
    success: bool = True                    # Operation succeeded?
    error: Optional[str] = None             # Error message (if failed)
    duration_ms: Optional[int] = None       # Operation duration
    metadata: Optional[Dict[str, Any]] = None  # Additional context

    def to_json(self) -> str:
        """Serialize to JSON line"""
        return json.dumps(asdict(self))

    @classmethod
    def from_json(cls, line: str) -> 'AuditEntry':
        """Deserialize from JSON line"""
        return cls(**json.loads(line))

# Example entries
examples = [
    AuditEntry(
        timestamp="2025-11-24T10:30:00.123Z",
        operation="write_lesson",
        path="books/ai-native-dev/lessons/01-intro.md",
        agent_id="claude-sonnet-4",
        book_id="ai-native-dev",
        user_id="user_abc123",
        success=True,
        duration_ms=234,
        metadata={
            "word_count": 1500,
            "previous_hash": "abc123",
            "new_hash": "def456"
        }
    ),

    AuditEntry(
        timestamp="2025-11-24T10:31:00.456Z",
        operation="generate_summary",
        path="books/ai-native-dev/lessons/01-intro.summary.md",
        agent_id="claude-sonnet-4",
        book_id="ai-native-dev",
        success=True,
        duration_ms=5678,
        metadata={
            "source_hash": "def456",
            "summary_length": 150,
            "model": "claude-sonnet-4"
        }
    ),

    AuditEntry(
        timestamp="2025-11-24T10:32:00.789Z",
        operation="upload_asset",
        path="books/ai-native-dev/assets/slides/ch01.pdf",
        agent_id="system",
        book_id="ai-native-dev",
        user_id="admin_xyz",
        success=True,
        duration_ms=1234,
        metadata={
            "size_bytes": 2400000,
            "mime_type": "application/pdf",
            "cdn_url": "https://cdn.example.com/slides/ch01.pdf"
        }
    )
]
```

### 3.4 Audit Manager Implementation

```python
from pathlib import Path
from datetime import datetime, date
from typing import List, Optional, Callable
import time

class AuditManager:
    """Manages audit logging with daily rotation"""

    def __init__(self, storage: StorageBackend, enabled: bool = True):
        self.storage = storage
        self.enabled = enabled

    def log(self, operation: str, path: str, agent_id: str, **kwargs) -> None:
        """Log an operation"""
        if not self.enabled:
            return

        entry = AuditEntry(
            timestamp=datetime.utcnow().isoformat() + "Z",
            operation=operation,
            path=path,
            agent_id=agent_id,
            **kwargs
        )

        # Daily rotation: .audit/YYYY-MM-DD.jsonl
        audit_path = f".audit/{date.today().isoformat()}.jsonl"

        # Append to existing log
        try:
            existing = self.storage.read(audit_path).decode('utf-8')
        except:
            existing = ""

        updated = existing + entry.to_json() + "\n"
        self.storage.write(audit_path, updated.encode('utf-8'))

    def track_operation(self, operation: str, path: str, agent_id: str, **kwargs):
        """Decorator to track operation duration and errors"""
        def decorator(func: Callable):
            def wrapper(*args, **func_kwargs):
                start_time = time.time()
                success = True
                error = None

                try:
                    result = func(*args, **func_kwargs)
                    return result
                except Exception as e:
                    success = False
                    error = str(e)
                    raise
                finally:
                    duration_ms = int((time.time() - start_time) * 1000)
                    self.log(
                        operation=operation,
                        path=path,
                        agent_id=agent_id,
                        success=success,
                        error=error,
                        duration_ms=duration_ms,
                        **kwargs
                    )
            return wrapper
        return decorator

    def query(
        self,
        start_date: Optional[date] = None,
        end_date: Optional[date] = None,
        operation: Optional[str] = None,
        agent_id: Optional[str] = None,
        book_id: Optional[str] = None,
        success: Optional[bool] = None,
        limit: int = 1000
    ) -> List[AuditEntry]:
        """Query audit logs with filters"""

        if start_date is None:
            start_date = date.today()
        if end_date is None:
            end_date = start_date

        entries = []
        current_date = start_date

        while current_date <= end_date and len(entries) < limit:
            audit_path = f".audit/{current_date.isoformat()}.jsonl"

            try:
                content = self.storage.read(audit_path).decode('utf-8')

                for line in content.strip().split('\n'):
                    if not line:
                        continue

                    entry = AuditEntry.from_json(line)

                    # Apply filters
                    if operation and entry.operation != operation:
                        continue
                    if agent_id and entry.agent_id != agent_id:
                        continue
                    if book_id and entry.book_id != book_id:
                        continue
                    if success is not None and entry.success != success:
                        continue

                    entries.append(entry)

                    if len(entries) >= limit:
                        break

            except:
                # Log file doesn't exist for this date
                pass

            current_date = date.fromordinal(current_date.toordinal() + 1)

        return entries[-limit:]  # Return most recent

    def get_stats(self, days: int = 7) -> dict:
        """Get aggregate statistics"""
        end_date = date.today()
        start_date = date.fromordinal(end_date.toordinal() - days)

        entries = self.query(start_date=start_date, end_date=end_date, limit=100000)

        stats = {
            "total_operations": len(entries),
            "successful": sum(1 for e in entries if e.success),
            "failed": sum(1 for e in entries if not e.success),
            "by_operation": {},
            "by_agent": {},
            "by_book": {},
            "avg_duration_ms": 0
        }

        durations = []
        for entry in entries:
            # By operation
            stats["by_operation"][entry.operation] = \
                stats["by_operation"].get(entry.operation, 0) + 1

            # By agent
            stats["by_agent"][entry.agent_id] = \
                stats["by_agent"].get(entry.agent_id, 0) + 1

            # By book
            if entry.book_id:
                stats["by_book"][entry.book_id] = \
                    stats["by_book"].get(entry.book_id, 0) + 1

            # Duration
            if entry.duration_ms:
                durations.append(entry.duration_ms)

        if durations:
            stats["avg_duration_ms"] = sum(durations) // len(durations)

        return stats
```

### 3.5 Usage Examples

```python
# Initialize
storage = StorageBackend("fs", root="./content")
audit = AuditManager(storage)

# Manual logging
audit.log(
    operation="write_lesson",
    path="books/ai-native-dev/lessons/01-intro.md",
    agent_id="claude-sonnet-4",
    book_id="ai-native-dev",
    metadata={"word_count": 1500}
)

# Decorator-based tracking
@audit.track_operation(
    operation="generate_summary",
    path="books/ai-native-dev/lessons/01-intro.summary.md",
    agent_id="claude-sonnet-4",
    book_id="ai-native-dev"
)
def generate_summary(content: str) -> str:
    # This function's duration and errors are automatically logged
    return ai_generate_summary(content)

# Query logs
recent_writes = audit.query(
    operation="write_lesson",
    agent_id="claude-sonnet-4",
    limit=100
)

# Get statistics
stats = audit.get_stats(days=7)
print(f"Total operations: {stats['total_operations']}")
print(f"Success rate: {stats['successful'] / stats['total_operations'] * 100:.1f}%")
print(f"By operation: {stats['by_operation']}")
```

---

## 4. MCP Layer (Cloudflare Patterns)

### 4.1 Why Cloudflare MCP Architecture?

The **Cloudflare Workers Bindings MCP Server** demonstrates production-grade patterns:

**Key Patterns**:

- âœ… OAuth authentication (built-in)
- âœ… Metrics tracking (Analytics Engine)
- âœ… Error handling (Sentry-ready)
- âœ… Tool registration (modular)
- âœ… Type safety (Zod schemas)
- âœ… Remote MCP (SSE transport)

### 4.2 MCP Server Architecture

```python
from typing import Dict, Any, Callable, Optional
from dataclasses import dataclass
import json

@dataclass
class MCPToolSchema:
    """Schema for MCP tool definition"""
    name: str
    description: str
    input_schema: Dict[str, Any]
    handler: Callable

class PanaversityMCPServer:
    """MCP server for PanaversityFS"""

    def __init__(
        self,
        storage: StorageBackend,
        audit: AuditManager,
        metrics: Optional['MetricsTracker'] = None
    ):
        self.storage = storage
        self.audit = audit
        self.metrics = metrics
        self.tools: Dict[str, MCPToolSchema] = {}

    def tool(
        self,
        name: str,
        description: str,
        input_schema: Dict[str, Any]
    ):
        """Decorator to register MCP tool"""
        def decorator(handler: Callable):
            self.tools[name] = MCPToolSchema(
                name=name,
                description=description,
                input_schema=input_schema,
                handler=handler
            )
            return handler
        return decorator

    def list_tools(self) -> Dict[str, Any]:
        """List all available tools (MCP protocol)"""
        return {
            "tools": [
                {
                    "name": tool.name,
                    "description": tool.description,
                    "inputSchema": tool.input_schema
                }
                for tool in self.tools.values()
            ]
        }

    def call_tool(self, name: str, arguments: Dict[str, Any]) -> Dict[str, Any]:
        """Execute tool (MCP protocol)"""
        if name not in self.tools:
            return {
                "content": [{
                    "type": "text",
                    "text": f"Error: Unknown tool '{name}'"
                }],
                "isError": True
            }

        tool = self.tools[name]

        try:
            # Track metrics
            if self.metrics:
                self.metrics.track_tool_call(name)

            # Execute handler
            result = tool.handler(**arguments)

            # Return MCP response
            return {
                "content": [{
                    "type": "text",
                    "text": json.dumps(result) if isinstance(result, dict) else str(result)
                }]
            }

        except Exception as e:
            # Track error
            if self.metrics:
                self.metrics.track_error(name, str(e))

            return {
                "content": [{
                    "type": "text",
                    "text": f"Error: {str(e)}"
                }],
                "isError": True
            }
```

### 4.3 Metrics Tracking (Cloudflare Pattern)

```python
from datetime import datetime
from typing import Dict, Any

class MetricsTracker:
    """Metrics tracking (inspired by Cloudflare Analytics Engine)"""

    def __init__(self, storage: StorageBackend, server_name: str, version: str):
        self.storage = storage
        self.server_name = server_name
        self.version = version
        self.buffer = []

    def track_tool_call(
        self,
        tool_name: str,
        user_id: Optional[str] = None,
        duration_ms: Optional[int] = None
    ):
        """Track tool invocation"""
        self.buffer.append({
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "event": "tool_call",
            "tool_name": tool_name,
            "user_id": user_id,
            "duration_ms": duration_ms,
            "server_name": self.server_name,
            "version": self.version
        })

        if len(self.buffer) >= 100:
            self.flush()

    def track_error(self, tool_name: str, error: str):
        """Track error"""
        self.buffer.append({
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "event": "error",
            "tool_name": tool_name,
            "error": error,
            "server_name": self.server_name,
            "version": self.version
        })

        self.flush()  # Flush errors immediately

    def track_session_start(self, user_id: Optional[str] = None):
        """Track session initialization"""
        self.buffer.append({
            "timestamp": datetime.utcnow().isoformat() + "Z",
            "event": "session_start",
            "user_id": user_id,
            "server_name": self.server_name,
            "version": self.version
        })

    def flush(self):
        """Write buffered metrics to storage"""
        if not self.buffer:
            return

        metrics_path = f".metrics/{datetime.utcnow().strftime('%Y-%m-%d')}.jsonl"

        try:
            existing = self.storage.read(metrics_path).decode('utf-8')
        except:
            existing = ""

        lines = [json.dumps(m) for m in self.buffer]
        updated = existing + "\n".join(lines) + "\n"

        self.storage.write(metrics_path, updated.encode('utf-8'))
        self.buffer.clear()
```

---

## 5. Intelligence Layer (LanceDB)

### 5.1 Why LanceDB?

**LanceDB** is a serverless, embedded vector database that runs directly on object storage.

**Key Benefits**:

- âœ… Serverless (no managed instance needed)
- âœ… Runs on R2/S3 (collocated with content)
- âœ… Incremental indexing (delta updates)
- âœ… Multimodal (text + images)
- âœ… Fast search (< 100ms)

### 5.2 Vector Storage Architecture

```python
import lancedb
from typing import List, Dict, Any

class VectorManager:
    """Manages vector embeddings for RAG"""

    def __init__(self, storage: StorageBackend, book_id: str):
        self.storage = storage
        self.book_id = book_id

        # LanceDB runs on same storage backend
        self.db = lancedb.connect(f"s3://panaversity-books/.vectors/{book_id}.lance")

        # Create table if not exists
        if "content" not in self.db.table_names():
            self.table = self.db.create_table("content", schema={
                "path": "string",
                "content_type": "string",  # "lesson", "chapter", "part"
                "title": "string",
                "content": "string",
                "embedding": "vector[1536]",  # OpenAI ada-002 dimension
                "hash": "string",
                "indexed_at": "timestamp"
            })
        else:
            self.table = self.db.open_table("content")

    def index_content(self, path: str, content: str, content_type: str, title: str):
        """Index content for vector search"""
        import openai
        import hashlib
        from datetime import datetime

        # Generate embedding
        response = openai.Embedding.create(
            model="text-embedding-ada-002",
            input=content
        )
        embedding = response['data'][0]['embedding']

        # Content hash for tracking changes
        content_hash = hashlib.sha256(content.encode()).hexdigest()[:12]

        # Add to table
        self.table.add([{
            "path": path,
            "content_type": content_type,
            "title": title,
            "content": content[:10000],  # Store first 10k chars
            "embedding": embedding,
            "hash": content_hash,
            "indexed_at": datetime.utcnow()
        }])

    def search(self, query: str, limit: int = 10) -> List[Dict[str, Any]]:
        """Vector search for similar content"""
        import openai

        # Generate query embedding
        response = openai.Embedding.create(
            model="text-embedding-ada-002",
            input=query
        )
        query_embedding = response['data'][0]['embedding']

        # Search
        results = self.table.search(query_embedding).limit(limit).to_pandas()

        return results.to_dict('records')

    def needs_reindex(self, path: str, current_hash: str) -> bool:
        """Check if content needs reindexing"""
        results = self.table.search().where(f"path = '{path}'").to_pandas()

        if len(results) == 0:
            return True  # Not indexed yet

        return results.iloc[0]['hash'] != current_hash
```

### 5.3 Summary Generation Engine

```python
import anthropic
from typing import Optional

class SummaryEngine:
    """AI-powered summary generation"""

    def __init__(self, api_key: str):
        self.client = anthropic.Anthropic(api_key=api_key)

    def generate_lesson_summary(
        self,
        content: str,
        max_words: int = 150
    ) -> str:
        """Generate lesson summary"""

        message = self.client.messages.create(
            model="claude-sonnet-4",
            max_tokens=500,
            messages=[{
                "role": "user",
                "content": f"""Summarize this lesson in {max_words} words or less:

{content}

Requirements:
- Focus on key concepts and takeaways
- Use clear, concise language
- Suitable for a course overview page
"""
            }]
        )

        return message.content[0].text

    def generate_chapter_summary(
        self,
        lesson_summaries: List[str],
        chapter_title: str,
        max_words: int = 300
    ) -> str:
        """Generate chapter summary from lesson summaries"""

        combined = "\n\n".join([f"- {s}" for s in lesson_summaries])

        message = self.client.messages.create(
            model="claude-sonnet-4",
            max_tokens=800,
            messages=[{
                "role": "user",
                "content": f"""Summarize this chapter "{chapter_title}" based on its lesson summaries:

{combined}

Requirements:
- {max_words} words or less
- Synthesize main themes across lessons
- Explain learning progression
"""
            }]
        )

        return message.content[0].text
```

---

## 6. Multi-Book Registry

### 6.1 Registry Schema

```yaml
# registry.yaml - Master book registry
version: "1.0"

# Storage configuration
storage:
  backend: "r2" # Active backend

  backends:
    local:
      type: "fs"
      root: "./content"

    r2:
      type: "s3"
      bucket: "panaversity-books"
      region: "auto"
      endpoint: "${R2_ENDPOINT}"
      access_key_id: "${R2_ACCESS_KEY_ID}"
      secret_access_key: "${R2_SECRET_ACCESS_KEY}"

    supabase:
      type: "s3"
      bucket: "books"
      endpoint: "${SUPABASE_ENDPOINT}/storage/v1/s3"
      access_key_id: "${SUPABASE_ACCESS_KEY_ID}"
      secret_access_key: "${SUPABASE_SECRET_ACCESS_KEY}"

# Registered books
books:
  - id: "ai-native-software-development"
    slug: "ai-native-dev"
    title: "AI-Native Software Development"
    status: "active" # active, archived, draft
    vertical: "technical_education"
    created_at: "2025-01-15T00:00:00Z"
    updated_at: "2025-11-24T10:00:00Z"

    metadata:
      parts_count: 13
      chapters_count: 84
      lessons_count: 450
      assets_count: 120
      total_size_bytes: 150000000

    features:
      auto_summary: true
      vector_search: true
      asset_optimization: true
      audit_logging: true

    maintainers:
      - user_id: "admin_xyz"
        role: "owner"
      - user_id: "editor_abc"
        role: "editor"

  - id: "python-fundamentals"
    slug: "python-101"
    title: "Python Fundamentals"
    status: "active"
    vertical: "programming"
    created_at: "2025-02-01T00:00:00Z"
    updated_at: "2025-11-20T15:00:00Z"

    metadata:
      parts_count: 5
      chapters_count: 30
      lessons_count: 150
      assets_count: 45
      total_size_bytes: 50000000

    features:
      auto_summary: true
      vector_search: false # Not enabled yet
      asset_optimization: true
      audit_logging: true

# Global settings
observability:
  audit_retention_days: 90
  metrics_retention_days: 365
  metrics_enabled: true
  error_tracking_enabled: true

  alert_webhooks:
    - name: "slack_alerts"
      url: "${SLACK_WEBHOOK_URL}"
      events: ["error", "high_latency"]

    - name: "pagerduty"
      url: "${PAGERDUTY_WEBHOOK_URL}"
      events: ["critical_error"]

# Rate limiting (per-agent)
rate_limits:
  default:
    requests_per_minute: 60
    requests_per_hour: 1000

  premium_agents:
    requests_per_minute: 300
    requests_per_hour: 10000

# Backup configuration
backups:
  enabled: true
  schedule: "0 2 * * *" # Daily at 2am UTC
  retention_days: 30
  destinations:
    - "s3://panaversity-backups/books/"
```

### 6.2 Registry Manager

```python
import yaml
from typing import List, Optional, Dict, Any
from dataclasses import dataclass, asdict
from datetime import datetime

@dataclass
class BookInfo:
    """Book information"""
    id: str
    slug: str
    title: str
    status: str
    vertical: str
    created_at: str
    updated_at: str
    metadata: Dict[str, Any]
    features: Dict[str, bool]
    maintainers: List[Dict[str, str]]

class RegistryManager:
    """Manages multi-book registry"""

    def __init__(self, storage: StorageBackend):
        self.storage = storage
        self._registry = None

    def load(self) -> Dict[str, Any]:
        """Load registry from storage"""
        content = self.storage.read("registry.yaml").decode('utf-8')
        self._registry = yaml.safe_load(content)
        return self._registry

    def save(self):
        """Save registry to storage"""
        content = yaml.dump(self._registry, default_flow_style=False)
        self.storage.write("registry.yaml", content.encode('utf-8'))

    def list_books(self, status: Optional[str] = None) -> List[BookInfo]:
        """List all books"""
        if not self._registry:
            self.load()

        books = []
        for book_data in self._registry['books']:
            if status and book_data['status'] != status:
                continue
            books.append(BookInfo(**book_data))

        return books

    def get_book(self, book_id: str) -> Optional[BookInfo]:
        """Get book by ID"""
        if not self._registry:
            self.load()

        for book_data in self._registry['books']:
            if book_data['id'] == book_id:
                return BookInfo(**book_data)

        return None

    def add_book(self, book: BookInfo) -> None:
        """Add new book to registry"""
        if not self._registry:
            self.load()

        # Check if already exists
        if any(b['id'] == book.id for b in self._registry['books']):
            raise ValueError(f"Book '{book.id}' already exists")

        self._registry['books'].append(asdict(book))
        self.save()

    def update_book(self, book_id: str, updates: Dict[str, Any]) -> None:
        """Update book metadata"""
        if not self._registry:
            self.load()

        for book in self._registry['books']:
            if book['id'] == book_id:
                book.update(updates)
                book['updated_at'] = datetime.utcnow().isoformat() + "Z"
                self.save()
                return

        raise ValueError(f"Book '{book_id}' not found")

    def delete_book(self, book_id: str) -> None:
        """Remove book from registry"""
        if not self._registry:
            self.load()

        self._registry['books'] = [
            b for b in self._registry['books'] if b['id'] != book_id
        ]
        self.save()

    def get_stats(self) -> Dict[str, Any]:
        """Get registry statistics"""
        books = self.list_books()

        return {
            "total_books": len(books),
            "active_books": len([b for b in books if b.status == "active"]),
            "total_parts": sum(b.metadata.get('parts_count', 0) for b in books),
            "total_chapters": sum(b.metadata.get('chapters_count', 0) for b in books),
            "total_lessons": sum(b.metadata.get('lessons_count', 0) for b in books),
            "total_assets": sum(b.metadata.get('assets_count', 0) for b in books),
            "total_size_bytes": sum(b.metadata.get('total_size_bytes', 0) for b in books)
        }
```

---

## 7. Book Schema

### 7.1 Book Specification Format

```yaml
# books/ai-native-software-development/book.yaml
schema_version: "1.2"

metadata:
  id: "ai-native-software-development"
  title: "AI-Native Software Development"
  subtitle: "Building Production Software in the Age of AI Agents"
  authors:
    - name: "Panaversity Team"
      role: "primary"
  version: "2.0.0"
  language: "en"
  vertical: "technical_education"
  created_at: "2025-01-15T00:00:00Z"
  updated_at: "2025-11-24T10:00:00Z"

  # Target audience
  audience:
    level: "intermediate" # beginner, intermediate, advanced
    prerequisites:
      - "Basic programming knowledge"
      - "Familiarity with command line"

  # Learning outcomes
  outcomes:
    - "Build production applications using AI coding agents"
    - "Apply Spec-Driven Development methodology"
    - "Deploy AI-native applications to cloud platforms"

# Book structure
structure:
  parts:
    - id: "01-Introducing-AI-Driven-Development"
      order: 1
      slug: "introducing-ai"
      title: "Part 1: Introducing AI-Driven Development"
      source_path: "parts/01-Introducing-AI-Driven-Development/README.md"
      summary_path: "parts/01-Introducing-AI-Driven-Development/summary.md"
      metadata_path: "parts/01-Introducing-AI-Driven-Development/metadata.json"

      # Vertical intelligence settings
      vertical:
        summary_enabled: true
        vector_indexed: true
        last_validated: "2025-11-24T09:00:00Z"

      chapters:
        - id: "01-ai-development-revolution"
          order: 1
          slug: "ai-revolution"
          title: "Chapter 1: The AI Development Revolution"
          source_path: "chapters/01-ai-development-revolution/README.md"
          summary_path: "chapters/01-ai-development-revolution/summary.md"

          vertical:
            summary_enabled: true
            vector_indexed: true

          lessons:
            - id: "01-moment_that_changed_everything"
              order: 1
              slug: "moment-changed-everything"
              title: "The Moment That Changed Everything"
              source_path: "lessons/01-moment_that_changed_everything.md"
              summary_path: "lessons/01-moment_that_changed_everything.summary.md"

              # Lesson metadata
              estimated_minutes: 15
              difficulty: "beginner"

              # Assets
              assets:
                - type: "slide"
                  path: "assets/slides/chapter-01-slides.pdf"
                  cdn_url: "https://cdn.example.com/slides/ch01.pdf"

                - type: "image"
                  path: "assets/images/ai-revolution-timeline.png"
                  cdn_url: "https://cdn.example.com/img/timeline.png"

              # Vertical intelligence
              vertical:
                summary_enabled: true
                summary_hash: "abc123..." # Content hash
                vector_indexed: true
                vector_id: "v_abc123..."
                last_validated: "2025-11-24T08:00:00Z"
                validation_score: 0.95

            - id: "02-three-trillion-developer-economy"
              order: 2
              slug: "developer-economy"
              title: "The $3 Trillion Developer Economy"
              source_path: "lessons/02-three-trillion-developer-economy.md"
              # ... similar structure

        - id: "02-ai-turning-point"
          order: 2
          # ... similar structure

# Asset configuration
assets:
  slides:
    format: "pdf"
    optimization: "none"
    cdn_enabled: true
    max_size_mb: 10

  images:
    formats: ["png", "jpg", "webp"]
    optimization: "webp" # Convert to WebP
    max_width: 1920
    max_height: 1080
    cdn_enabled: true

  videos:
    formats: ["mp4", "webm"]
    optimization: "h264" # Transcode to H.264
    max_resolution: "1080p"
    max_size_mb: 100
    cdn_enabled: true

  audio:
    formats: ["mp3", "m4a"]
    optimization: "mp3"
    max_bitrate: "192k"
    cdn_enabled: true

# Vertical intelligence settings
intelligence:
  summary:
    enabled: true
    model: "claude-sonnet-4"
    max_words_lesson: 150
    max_words_chapter: 300
    max_words_part: 500
    regenerate_on_change: true

  vector_search:
    enabled: true
    model: "text-embedding-ada-002"
    dimension: 1536
    chunk_size: 1000
    chunk_overlap: 200
    reindex_on_change: true

  validation:
    enabled: true
    checks:
      - "broken_links"
      - "missing_assets"
      - "proficiency_alignment"
      - "content_consistency"
    schedule: "0 3 * * *" # Daily at 3am UTC

# Publishing settings
publishing:
  docusaurus:
    enabled: true
    base_url: "https://learn.panaversity.com"
    repo_url: "https://github.com/panaversity/ai-native-dev"

    sidebar:
      auto_generate: true
      collapse_single_children: true

    theme:
      primary_color: "#0066cc"
      code_theme: "github-dark"

  epub:
    enabled: false

  pdf:
    enabled: false
```

---

_[Continued in next part due to length...]_

**Status**: Part 1 of specification created (9,000+ words so far)

**Next sections to write**:

- Part 3: MCP Tools (13 detailed tool implementations)
- Part 4: Implementation (code structure, components)
- Part 5: Operations (deployment, monitoring)
- Part 6: Integration (Docusaurus, CI/CD)
- Part 7: Roadmap (phased rollout)

Should I continue with the remaining sections? This will result in a ~20,000-word comprehensive specification document.
