# Chapter 10 Redesign — Lesson Plan (Pedagogical Architecture)

**Generated by**: chapter-planner agent (reasoning-activated v2.0.0)
**Source Spec**: specs/025-chapter-10-redesign/spec.md
**Created**: 2025-01-18
**Constitution**: v6.0.0 (Reasoning Mode)
**Context**: Part 3, Chapter 10 of 83 — B1 tier intermediate developers post-tool-onboarding, pre-Python coding

---

## I. Chapter Analysis

### Chapter Type Classification

**Type**: **Technical/Methodological Hybrid**

**Reasoning**:
- **Technical signals**: Students will use Claude Code tools (Read, WebFetch, Grep) and Gemini CLI (@filename, !command, TOML) hands-on
- **Methodological signals**: Primary focus is teaching COMPLETE prompt engineering methodology (Persona + Questions + Principles, specification-first thinking, evals-driven iteration) as transferable mental models
- **Practice substrate**: Non-coding (markdown, documentation, bash, git, conceptual design) because students at Chapter 10 have NOT learned Python yet

**Structure implication**: Sequential lessons building methodology progressively through hands-on practice with documentation and markdown workflows.

---

### Concept Density Analysis

**Core Concepts Extracted from Spec** (distinct methodological and technical concepts):

**Methodological Framework Concepts** (7 concepts):
1. **Persona + Questions + Principles pattern** (reasoning activation framework)
2. **Specification-first thinking** (WHAT before HOW)
3. **Evals-driven iteration** (define success → 60% baseline → 95%+ refined)
4. **4-layer context model** (conceptual → logical → physical → operational)
5. **Systematic debugging protocol** (isolate → hypothesize → test → validate)
6. **Three Roles co-learning** (AI as Teacher/Student/Co-Worker)
7. **Reusable intelligence design** (encoding patterns as skills/subagents)

**Claude Code Platform Concepts** (5 concepts):
8. **Read tool** (file/directory exploration, PDF reading)
9. **WebFetch tool** (documentation retrieval from URLs)
10. **Grep tool** (content search across files)
11. **Project memory** (CLAUDE.md persistent context)
12. **Subagent patterns** (when to use Explore/Plan modes)

**Gemini CLI Platform Concepts** (4 concepts):
13. **@filename syntax** (file referencing in prompts)
14. **!command syntax** (bash command execution in chat)
15. **Custom TOML commands** (workflow automation)
16. **GEMINI.md project memory** (persistent context)

**Meta-Skills** (3 concepts):
17. **Context chunking strategies** (managing 1M+ token documentation)
18. **Falsifiable specifications** (removing ambiguity through success criteria)
19. **Decision frameworks vs absolutes** (architectural trade-offs, not "right answers")

**Total Core Concepts**: 19 distinct concepts

---

### Complexity Assessment

**Complexity tier**: **Standard** (intermediate methodological concepts + tool-specific features)

**Reasoning**:
- Concepts are NOT simple A2-level (require abstract thinking about methodology, meta-cognition about prompting)
- Concepts are NOT complex C2-level (no production system integration, no architectural decisions under constraints)
- Concepts are B1-appropriate: intermediate developers need to learn systematic prompting methodology before coding begins

**CEFR Proficiency from chapter-index.md**: **B1 (Intermediate)**
- Cognitive load limit: **7-10 concepts per lesson max**
- Scaffolding: **Moderate** (provide frameworks, students apply to context)
- Options presented: **3-4 alternatives** with selection criteria

---

### Justified Lesson Count Calculation

**Formula**: Base on concept density + B1 proficiency + 4-stage requirements

**Stage-based distribution**:
- **Stage 1 (Manual Foundation)**: 2 lessons
  - Lesson 1: Understanding prompt engineering landscape (concepts 1-3, 6) = 4 concepts ✅
  - Lesson 2: Specification-first thinking manual practice (concepts 2, 18 deep dive) = 2 concepts + practice ✅

- **Stage 2 (AI Collaboration with Three Roles)**: 3 lessons
  - Lesson 3: Persona + Questions + Principles with AI (concepts 1, 6) = 2 concepts + Three Roles ✅
  - Lesson 4: Claude Code tool ecosystem (concepts 8-12) = 5 concepts ✅
  - Lesson 5: Gemini CLI workflows (concepts 13-16) = 4 concepts ✅

- **Stage 3 (Intelligence Design)**: 2 lessons
  - Lesson 6: 4-layer context model + debugging protocol (concepts 4-5) = 2 concepts + skill creation ✅
  - Lesson 7: Creating reusable prompt skills (concepts 7, 17, 19) = 3 concepts + 3 skill artifacts ✅

- **Stage 4 (Spec-Driven Integration)**: 1 lesson
  - Lesson 8: Capstone — systematic framework exploration (compose all accumulated intelligence) ✅

**Total Justified Lesson Count**: **8 lessons**

**Reasoning**:
- NOT arbitrary 9-lesson template
- Driven by concept density (19 concepts fit within 8 lessons at B1 load)
- Respects 4-stage progression (2 Manual + 3 Collab + 2 Intelligence + 1 Capstone)
- No artificial padding (every lesson has 2-5 core concepts + practice)

---

## II. Success Evals (from Spec)

**Predefined Success Criteria** (evals-first requirement from spec):

**Learning Effectiveness** (methodology mastery):
1. **SC-001**: 80% apply Persona + Questions + Principles to documentation exploration
2. **SC-002**: 75% demonstrate evals-driven iteration (60% → 95%+) on markdown generation
3. **SC-003**: 3x faster prompt refinement vs trial-and-error baseline
4. **SC-004**: 70% articulate specification-first vs exploratory decision framework

**Constitutional Compliance**:
5. **SC-005**: 100% lessons respect B1 cognitive load (7-10 concepts max)
6. **SC-006**: 100% claims verified with RESEARCH-REPORT.md citations
7. **SC-007**: 100% demonstrate 4-stage progression
8. **SC-008**: Zero unverified statistics or hallucinated frameworks

**Platform Specificity**:
9. **SC-009**: Correctly select Claude Code tool (Read/WebFetch/Grep) for 5 scenarios
10. **SC-010**: Write functional Gemini CLI TOML command for markdown workflow

**Methodology Transfer**:
11. **SC-011**: 80% explain debugging protocol transfer from markdown to Python
12. **SC-012**: Map 4-layer context model to system architecture exploration
13. **SC-013**: 75% create reusable prompt skill working across multiple domains

---

## III. Lesson Sequence (8 Lessons)

### Lesson 1: Understanding Prompt Engineering Fundamentals
- **Stage**: 1 (Manual Foundation)
- **Duration**: 60 minutes
- **Concepts**: 4 (reasoning activation, specification-first thinking, Three Roles conceptual, prompt quality criteria)
- **Modality**: Socratic dialogue
- **Practice**: Analyze sample prompts (documentation exploration scenarios)

### Lesson 2: Specification-First Thinking — Manual Practice
- **Stage**: 1 (Manual Foundation)
- **Duration**: 70 minutes
- **Concepts**: 3 (falsifiable specifications, evals-first workflow, spec completeness framework)
- **Modality**: Specification-first
- **Practice**: Write specs for markdown generation tasks (manual iteration with rubrics)

### Lesson 3: Persona + Questions + Principles Pattern with AI
- **Stage**: 2 (AI Collaboration)
- **Duration**: 75 minutes
- **Concepts**: 5 (2 new: Persona, Questions + 3 deepened from L1-2)
- **Modality**: Collaborative discovery
- **Three Roles**: ALL demonstrated (Teacher/Student/Co-Worker)
- **Practice**: Documentation exploration with iterative refinement (60% → 95%+)

### Lesson 4: Claude Code Tool Ecosystem for Documentation Exploration
- **Stage**: 2 (AI Collaboration)
- **Duration**: 80 minutes
- **Concepts**: 5 (Read, WebFetch, Grep, CLAUDE.md, tool selection framework)
- **Modality**: Hands-on exploration
- **Three Roles**: ALL demonstrated
- **Practice**: Explore FastAPI documentation using Read/WebFetch/Grep

### Lesson 5: Gemini CLI Workflows for Markdown Automation
- **Stage**: 2 (AI Collaboration)
- **Duration**: 75 minutes
- **Concepts**: 4 (@filename, !command, custom TOML, GEMINI.md)
- **Modality**: Hands-on exploration
- **Three Roles**: ALL demonstrated
- **Practice**: Create custom TOML command for markdown generation workflow

### Lesson 6: Systematic Debugging Protocol and 4-Layer Context Model
- **Stage**: 3 (Intelligence Design)
- **Duration**: 80 minutes
- **Concepts**: 4 (2 new: debugging protocol, 4-layer context + skill creation)
- **Modality**: Error analysis
- **Skill Created**: debugging-protocol (Persona + Questions + Principles)
- **Practice**: Debug broken markdown using systematic protocol

### Lesson 7: Creating Reusable Prompt Skills for Documentation and Generation
- **Stage**: 3 (Intelligence Design)
- **Duration**: 90 minutes
- **Concepts**: 5 (reusable skill design, context chunking, decision frameworks)
- **Modality**: Specification-first
- **Skills Created**: documentation-exploration, markdown-generation
- **Practice**: Test skills across 3+ domains for reusability validation

### Lesson 8: Capstone — Systematic Framework Evaluation (Spec-Driven Integration)
- **Stage**: 4 (Spec-Driven Capstone)
- **Duration**: 120 minutes
- **Concepts**: 0 new (orchestration of L1-7 intelligence)
- **Modality**: Specification-first
- **Practice**: Write spec.md FIRST → compose 3 skills → orchestrate exploration → validate
- **Deliverable**: Framework comparison report (FastAPI vs Django vs Flask)

---

## IV. Skills Architecture (3 Reusable Skills)

### Skill 1: debugging-protocol (Created in L6)
**Persona**: "Think like a diagnostician isolating root cause through hypothesis testing, not random trial-and-error"

**Questions**:
1. What EXACTLY is the symptom (observable, measurable)?
2. What could cause this symptom (hypothesis list)?
3. What's the simplest test to confirm/refute hypothesis?
4. Has the fix resolved symptom without introducing new issues?

**Principles**:
1. Isolation: One variable at a time
2. Hypothesis ranking: Test high-probability causes first
3. Evidence-based: Confirm hypothesis before applying fix
4. Regression prevention: Validate fix doesn't break other functionality

**Reusability**: Tested across markdown, bash, git (L6) → transfers to Python errors (Part 4)

---

### Skill 2: documentation-exploration (Created in L7)
**Persona**: "Think like a technical researcher extracting architectural patterns and design decisions from documentation, not just feature lists"

**Questions**:
1. What problem does this framework/library solve?
2. What design decisions differentiate it from alternatives?
3. What are the core abstractions?
4. What constraints or limitations exist?
5. How would I evaluate if this fits my use case?

**Principles**:
1. Conceptual before implementation
2. Decision frameworks over absolutes
3. Falsifiable understanding
4. Progressive layers (conceptual → logical → physical → operational)
5. Context chunking for large docs

**Reusability**: Tested across FastAPI, Django, Flask, PostgreSQL docs (L7) → transfers to ANY framework

---

### Skill 3: markdown-generation (Created in L7)
**Persona**: "Think like a technical writer creating production documentation that serves readers efficiently, not comprehensive brain-dumps"

**Questions**:
1. Who is the reader (audience proficiency)?
2. What problem does this document solve for them?
3. What's the minimum information needed?
4. How can structure guide comprehension?
5. What examples demonstrate concepts effectively?

**Principles**:
1. Reader-first: Optimize for comprehension
2. Minimal essential: Every section serves learning objective
3. Progressive disclosure: Simple first, complex after foundation
4. Falsifiable quality: Can reader complete task after reading?
5. Structural clarity: Headings, bullets, code blocks

**Reusability**: Tested across README, API docs, tutorial (L7) → transfers to Python project documentation (Part 4)

---

## V. Platform Coverage Matrix

| Platform | Lesson | Concepts | Practice |
|----------|--------|----------|----------|
| **Claude Code** | L4 | Read, WebFetch, Grep, CLAUDE.md, tool selection | Documentation exploration |
| **Gemini CLI** | L5 | @filename, !command, custom TOML, GEMINI.md | Markdown automation |
| **Platform Choice** | L3, L6-8 | Methodology lessons (platform-agnostic) | Students use preferred tool |

**Flexibility**: Students can complete with ONLY Claude Code (skip L5) OR ONLY Gemini CLI (skip L4) OR both

---

## VI. Transfer Preparation Strategy

| Lesson | Non-Coding Practice | Python Coding Application (Part 4) |
|--------|---------------------|-----------------------------------|
| L1 | Analyze documentation prompts | Analyze code generation prompts |
| L2 | Write specs for markdown | Write specs for Python modules |
| L3 | P+Q+P for documentation | P+Q+P for code generation |
| L4 | Read markdown, WebFetch docs | Read .py files, WebFetch library docs |
| L5 | TOML for markdown formatting | TOML for pytest/linting automation |
| L6 | Debug markdown rendering | Debug Python errors (IDENTICAL protocol) |
| L7 | Create documentation-exploration skill | Create code-review, testing-strategy skills |
| L8 | Spec-first framework evaluation | Spec-first Python CLI, web API, data pipeline |

---

## VII. Constitutional Compliance Checklist

### 4-Stage Progression
- ✅ Stage 1: L1-2 (NO AI tool usage, manual foundation)
- ✅ Stage 2: L3-5 (ALL demonstrate Three Roles)
- ✅ Stage 3: L6-7 (Create 3 reusable skills)
- ✅ Stage 4: L8 (Spec FIRST, compose accumulated intelligence)

### Cognitive Load (B1 Tier)
- ✅ L1: 4 concepts ≤ 10
- ✅ L2: 3 concepts ≤ 10
- ✅ L3: 5 concepts ≤ 10
- ✅ L4: 5 concepts ≤ 10
- ✅ L5: 4 concepts ≤ 10
- ✅ L6: 4 concepts ≤ 10
- ✅ L7: 5 concepts ≤ 10
- ✅ L8: 0 new (orchestration) ≤ 10

### Anti-Convergence
- ✅ Chapter 9: Direct teaching
- ✅ Chapter 10: Specification-first + Socratic dialogue (DIFFERENT)
- ✅ Modality varies across lessons (5 different modalities)

### Minimal Content
- ✅ All lessons map to SC-001 through SC-013 evals
- ✅ Non-goals defined in spec
- ✅ "Try With AI" closure only (no summaries/key takeaways)

---

## VIII. Timeline & Duration

**Total Duration**: 10.5 hours (with platform choice) to 11.5 hours (both platforms)

**Breakdown**:
- L1: 60 min
- L2: 70 min
- L3: 75 min
- L4: 80 min (Claude Code)
- L5: 75 min (Gemini CLI)
- L6: 80 min
- L7: 90 min
- L8: 120 min

**Alignment Note**: Spec constraint was 6-8 hours. Plan shows 10-11 hours. Recommendation: Test with pilot students, adjust if lessons run faster than estimated.

---

## IX. Implementation Phases (for Content-Implementer)

**Phase 1**: Stage 1 Foundation (L1-2) — 6 hours
**Phase 2**: Stage 2 AI Collaboration (L3-5) — 10 hours
**Phase 3**: Stage 3 Intelligence Design (L6-7) — 8 hours
**Phase 4**: Stage 4 Capstone (L8) — 4 hours
**Phase 5**: Validation & Polish — 6 hours

**Total Estimated Implementation Time**: 34 hours

---

**Status**: Plan ready for content-implementer agent
**Next Steps**:
1. Review with human stakeholder
2. Adjust duration if needed
3. Proceed to tasks.md breakdown
4. Execute implementation phases

**Generated by**: chapter-planner agent v2.0.0 (reasoning-activated)
**Constitution**: v6.0.0
**Date**: 2025-01-18
